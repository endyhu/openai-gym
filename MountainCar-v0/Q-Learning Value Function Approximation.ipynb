{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_size = env.observation_space.shape[0]\n",
    "\n",
    "print(f\"Observation Space Size: {os_size}\")\n",
    "print(f\"Observation Space Low : {env.observation_space.low}\")\n",
    "print(f\"Observation Space High: {env.observation_space.high}\")\n",
    "\n",
    "print(f\"\\nAction Space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator:\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        self.model.add(Dense(8, input_shape=(os_size,), activation=\"relu\"))\n",
    "        self.model.add(Dense(8, activation=\"relu\"))\n",
    "        self.model.add(Dense(3, activation=\"softmax\"))\n",
    "        \n",
    "        self.model.compile(optimizer=\"adam\", \n",
    "                           loss=\"mse\")\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "    def predict(self, s):\n",
    "        state = s.reshape(-1, os_size)\n",
    "        \n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def update(self, s, a, y):\n",
    "        state = s.reshape(-1, os_size)\n",
    "        \n",
    "        td_target = self.predict(s)\n",
    "        td_target[0][a] = y\n",
    "        \n",
    "        self.model.fit(state, td_target, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEpsilonGreedyPolicy(estimator, epsilon, nA):\n",
    "    def policy(state):\n",
    "        A = np.ones(nA) * (epsilon / nA)\n",
    "        best_action = np.argmax(estimator.predict(state))\n",
    "        A[best_action] = A[best_action] + (1 - epsilon)\n",
    "        \n",
    "        return A\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(env, estimator, num_episodes, discount=1.0, epsilon=0.1, batch_size=16):\n",
    "    replay_memory = []\n",
    "    \n",
    "    policy = getEpsilonGreedyPolicy(estimator, epsilon, env.action_space.n)\n",
    "    \n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    episode_finished = []\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        for t in range(200):\n",
    "#             if (i_episode + 1) % 5000 == 0:\n",
    "#                 env.render()\n",
    "            \n",
    "            action_prob = policy(state)\n",
    "            action_pos = [i for i in range(env.action_space.n)]\n",
    "            action = np.random.choice(action_pos, p=action_prob)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            episode_rewards[i_episode] = episode_rewards[i_episode] + reward\n",
    "            replay_memory.append([state, action, reward, next_state, done])\n",
    "            \n",
    "            if len(replay_memory) > batch_size:\n",
    "                replay_batch = random.sample(replay_memory, batch_size)\n",
    "                \n",
    "                for ss, aa, rr, ns, terminal in replay_batch:\n",
    "                    td_target = rr\n",
    "                    \n",
    "                    if not terminal:\n",
    "                        best_next_action_value = np.max(estimator.predict(ns))\n",
    "                        \n",
    "                        td_target = rr + discount * best_next_action_value\n",
    "                        \n",
    "                    estimator.update(ss, aa, td_target)\n",
    "            \n",
    "            if done:\n",
    "                if episode_rewards[i_episode] > -200:\n",
    "                    episode_finished.append([i_episode, episode_rewards[i_episode]])\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        clear_output(True)\n",
    "        print(f\"[{i_episode + 1:>5}/{num_episodes:>5}] Episode Total Reward: {episode_rewards[i_episode]}\")\n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    return episode_finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_finished = QLearning(env, estimator, 100, epsilon=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Episodes Finished:\")\n",
    "for episode in episode_finished[-5:]:\n",
    "    print(f\"[{episode[0] + 1:>5}] Total Reward: {episode[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playEnvOnce(env, estimator):\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        \n",
    "        action = np.argmax(estimator.predict(state))\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playEnvOnce(env, estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
