{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Size: 4\n",
      "Observation Space Low : [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "Observation Space High: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "\n",
      "Action Space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "os_size = env.observation_space.shape[0]\n",
    "\n",
    "print(f\"Observation Space Size: {os_size}\")\n",
    "print(f\"Observation Space Low : {env.observation_space.low}\")\n",
    "print(f\"Observation Space High: {env.observation_space.high}\")\n",
    "\n",
    "print(f\"\\nAction Space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator:\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        self.model.add(Dense(16, input_shape=(os_size,), activation=\"relu\"))\n",
    "        self.model.add(Dense(16, activation=\"relu\"))\n",
    "        self.model.add(Dense(env.action_space.n, activation=\"linear\"))\n",
    "        \n",
    "        self.model.compile(optimizer=\"adam\", \n",
    "                           loss=\"mse\")\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "    def predict(self, s):\n",
    "        state = s.reshape(-1, os_size)\n",
    "        \n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def update(self, s, a, y):\n",
    "        state = s.reshape(-1, os_size)\n",
    "        \n",
    "        td_target = self.predict(s)\n",
    "        td_target[0][a] = y\n",
    "        \n",
    "        self.model.fit(state, td_target, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\endyh\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\endyh\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 386\n",
      "Trainable params: 386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "estimator = Estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEpsilonGreedyPolicy(estimator, epsilon, nA):\n",
    "    def policy(state):\n",
    "        A = np.ones(nA) * (epsilon / nA)\n",
    "        best_action = np.argmax(estimator.predict(state))\n",
    "        A[best_action] = A[best_action] + (1 - epsilon)\n",
    "        \n",
    "        return A\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(env, estimator, num_episodes, discount=1.0, epsilon=0.1, batch_size=16):\n",
    "    replay_memory = []\n",
    "    \n",
    "    policy = getEpsilonGreedyPolicy(estimator, epsilon, env.action_space.n)\n",
    "    \n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        for t in range(200):\n",
    "#             if (i_episode + 1) % 5000 == 0:\n",
    "#                 env.render()\n",
    "            \n",
    "            action_prob = policy(state)\n",
    "            action_pos = [i for i in range(env.action_space.n)]\n",
    "            action = np.random.choice(action_pos, p=action_prob)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            episode_rewards[i_episode] = episode_rewards[i_episode] + reward\n",
    "            replay_memory.append([state, action, reward, next_state, done])\n",
    "            \n",
    "            if len(replay_memory) > batch_size:\n",
    "                replay_batch = random.sample(replay_memory, batch_size)\n",
    "                \n",
    "                for ss, aa, rr, ns, terminal in replay_batch:\n",
    "                    td_target = rr\n",
    "                    \n",
    "                    if not terminal:\n",
    "                        best_next_action_value = np.max(estimator.predict(ns))\n",
    "                        \n",
    "                        td_target = rr + discount * best_next_action_value\n",
    "                        \n",
    "                    estimator.update(ss, aa, td_target)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        clear_output(True)\n",
    "        print(f\"[{i_episode + 1:>5}/{num_episodes:>5}] Episode Total Reward: {episode_rewards[i_episode]}\")\n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  100/  100] Episode Total Reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = QLearning(env, estimator, 100, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 5 Episodes:\n",
      "[    1] Total Reward: 200.0\n",
      "[    2] Total Reward: 200.0\n",
      "[    3] Total Reward: 200.0\n",
      "[    4] Total Reward: 200.0\n",
      "[    5] Total Reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Last 5 Episodes:\")\n",
    "for i, reward in enumerate(episode_rewards[-5:]):\n",
    "    print(f\"[{i + 1:>5}] Total Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playEnvOnce(env, estimator):\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        \n",
    "        action = np.argmax(estimator.predict(state))\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "playEnvOnce(env, estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
